{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJsQokBvryUl",
    "outputId": "c29102b4-c437-44d8-9a55-1e7ca87e3b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaUYTf6LsAug"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niJdzqc2syBl"
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Tweets_data_NLP_Homework/Covid_19_tweets_train.csv',  engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2hN7aDqrFFH"
   },
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51w6KW7crFN8"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test):\n",
    "    prediction = model.predict(x_test)\n",
    "    # prediction_prob = model.predict_proba(x_test)\n",
    "    confusion = confusion_matrix(y_test, prediction)\n",
    "    print(\"Confusion matrix\",print_cm(confusion, ['Not pay', 'pay']))\n",
    "    print('Accuracy: {:.2f}'.format(accuracy_score(y_test, prediction)))\n",
    "    print('Precision: {:.2f}'.format(precision_score(y_test, prediction, average='macro')))\n",
    "    print('Recall: {:.2f}'.format(recall_score(y_test, prediction, average='macro')))\n",
    "    print('F1: {:.2f}'.format(f1_score(y_test, prediction, average='macro')))\n",
    "    print(classification_report(y_test, prediction))\n",
    "    # precision_recall_curve(y_test, prediction_prob)\n",
    "\n",
    "def clean_by_pattern(text, pattern):\n",
    "  '''repalcing a matched pattern in a text with a single space, and return a list of the text sentences.'''\n",
    "  cleaned_data = []\n",
    "  for d in text:\n",
    "    cleaned_data.append(re.sub(pattern,' ',str(d)))\n",
    "    \n",
    "  return cleaned_data\n",
    "\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywYLbi10s5hG"
   },
   "source": [
    "# Text Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RsFZgIMrlZt"
   },
   "source": [
    "Cleanning using regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJUWYZdks1Qu"
   },
   "outputs": [],
   "source": [
    "patterns = [r\"https://(\\S*)\", r\"@(\\S*)\", r\"[^a-zA-Z#]\", r\"\\s\\s+\", r'\\s+[a-zA-Z]\\s+']\n",
    "\n",
    "temp = []\n",
    "cleaned_tweets = []\n",
    "cleaned_tweets = clean_by_pattern(data[\"OriginalTweet\"], patterns[0]) # remove urls and images\n",
    "temp = clean_by_pattern(cleaned_tweets, patterns[1]) # remove mentions\n",
    "cleaned_tweets = clean_by_pattern(temp, patterns[2]) # remove the rest(numbers, punctuations, ....) except the hashtags.\n",
    "temp = clean_by_pattern(cleaned_tweets, patterns[3]) # replace multiple space (which we have after applying the previous pattern) with single space. \n",
    "cleaned_tweets = clean_by_pattern(temp, patterns[4]) # remove single characters (which we have after applying the previous pattern).\n",
    "# convert all tweets characters to lower case.\n",
    "temp = []\n",
    "for itr in range(0, len(cleaned_tweets)):\n",
    "    temp.append(cleaned_tweets[itr].lower())\n",
    "cleaned_tweets = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz0DcHxdrVxc"
   },
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vo5oPr6KdA0I",
    "outputId": "1aceba8a-2631-4421-f5b4-04c1252b8584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "['', 'advice talk neighbours family exchange phone numbers create contact list phone numbers neighbours schools employer chemist gp set online shopping accounts poss adequate supplies regular meds order']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stwd = stopwords.words('english')\n",
    "\n",
    "final_cleaned_tweets_tokens = []\n",
    "for tweet in cleaned_tweets:\n",
    "    sentence = [w for w in tweet.split() if w not in stwd] \n",
    "    # if sentence:\n",
    "    #     final_cleaned_tweets_tokens.append(sentence)\n",
    "    final_cleaned_tweets_tokens.append(sentence)\n",
    "final_cleaned_tweets_tokens[:3]\n",
    "final_cleaned_tweets = []\n",
    "for item in final_cleaned_tweets_tokens:\n",
    "    final_cleaned_tweets.append(\" \".join(item))\n",
    "print(final_cleaned_tweets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2vYddMotJm1"
   },
   "outputs": [],
   "source": [
    "# Add the cleaned tweets to the data\n",
    "final_cleaned_tweets_df = pd.DataFrame(final_cleaned_tweets)\n",
    "data['Cleaned Tweets'] = final_cleaned_tweets_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhMxE7LJrs8v"
   },
   "source": [
    "Encoding sentiments for trainning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOjzbgnneU9c"
   },
   "outputs": [],
   "source": [
    "# Find and Replace tech \n",
    "cleanup_nums = {\"Sentiment\": {\"Neutral\": 0, \"Positive\": 1, \"Negative\": 2, \"Extremely Positive\": 3,\n",
    "                                  \"Extremely Negative\": 4}}\n",
    "Labeled_data = data.replace(cleanup_nums)\n",
    "x = []\n",
    "y = []\n",
    "for key, item in data['Cleaned Tweets'].items():\n",
    "  if len(item):\n",
    "    x.append(item)\n",
    "    y.append(Labeled_data['Sentiment'][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WnDvyIpFny8l",
    "outputId": "87d9e6c7-297a-4f12-db21-9a6376c94086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        advice talk neighbours family exchange phone n...\n",
      "1        coronavirus australia woolworths give elderly ...\n",
      "2        food stock one empty please panic enough food ...\n",
      "3        ready go supermarket #covid outbreak paranoid ...\n",
      "4        news region first confirmed covid case came su...\n",
      "                               ...                        \n",
      "41126    airline pilots offering stock supermarket shel...\n",
      "41127    response complaint provided citing covid relat...\n",
      "41128    know getting tough rationing toilet paper #cor...\n",
      "41129    wrong smell hand sanitizer starting turn #coro...\n",
      "41130    well new used rift going amazon rn although no...\n",
      "Length: 41131, dtype: object\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        4\n",
      "4        1\n",
      "        ..\n",
      "41126    0\n",
      "41127    4\n",
      "41128    1\n",
      "41129    0\n",
      "41130    2\n",
      "Length: 41131, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x = pd.Series(x)\n",
    "y = pd.Series(y)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oh9aDbPLr0wM"
   },
   "source": [
    "# Splitting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thqOZzVAvszI",
    "outputId": "3c88538a-baaa-48ca-e72b-859f6a9d55d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (32904,) Val shape: (8227,)\n",
      "Train shape: (23032,) Val shape: (9872,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = x, y\n",
    "# create train and validation sets\n",
    "X_tr_, X_test, y_tr_, y_test = train_test_split(X, y, test_size=0.2, random_state=1000)\n",
    "\n",
    "# create train and validation sets from traning split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_tr_, y_tr_, test_size=0.3, random_state=1000)\n",
    "print('Train shape:', X_tr_.shape, 'Val shape:', X_test.shape)\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn2KotRHsCo8"
   },
   "source": [
    "# Bag-of-Word Vectorizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_N47wpBs6OF"
   },
   "source": [
    "create the transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_K2oRNV0sNJE",
    "outputId": "5eedf0ce-d562-4bcc-d84a-b74d16697b1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bow_vectorizer = CountVectorizer()\n",
    "\n",
    "Bow_vectorizer.fit(X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hOj0Ihos96h"
   },
   "source": [
    "Transforming data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuKLHHhYtAeW"
   },
   "outputs": [],
   "source": [
    "# transform training data\n",
    "X_tr_bow = Bow_vectorizer.transform(X_tr)\n",
    "\n",
    "# transform validation data\n",
    "X_val_bow = Bow_vectorizer.transform(X_val)\n",
    "\n",
    "# transform test data\n",
    "X_test_bow = Bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5P4yQOq8Uxcb"
   },
   "source": [
    "# BOW models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1AjZ_4IU0iu"
   },
   "outputs": [],
   "source": [
    "#Naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "NB_model_bow = GaussianNB().fit(X_tr_bow.toarray() ,y_tr)\n",
    "NB_predictions_bow = NB_model_bow.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCvyAXA9gCHh",
    "outputId": "393b984d-12dd-43cb-debc-9e710fb2c266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   340.0   143.0 \n",
      "        pay   308.0   357.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.26\n",
      "Precision: 0.29\n",
      "Recall: 0.29\n",
      "F1: 0.27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.23      0.27      1495\n",
      "           1       0.34      0.16      0.22      2228\n",
      "           2       0.30      0.22      0.25      1989\n",
      "           3       0.34      0.33      0.33      1398\n",
      "           4       0.17      0.50      0.25      1117\n",
      "\n",
      "    accuracy                           0.26      8227\n",
      "   macro avg       0.29      0.29      0.27      8227\n",
      "weighted avg       0.30      0.26      0.26      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Extremely Negative\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Extremely Negative\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Extremely Negative\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Neutral\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Extremely Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Neutral\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Neutral\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Neutral\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Extremely Negative\n"
     ]
    }
   ],
   "source": [
    "dict = {0 : \"Neutral\", 1 : \"Positive\", 2 : \"Negative\", 3 : \"Extremely Positive\",\n",
    "                                  4 : \"Extremely Negative\"}\n",
    "                                  \n",
    "evaluate(NB_model_bow, X_test_bow.toarray(), y_test)\n",
    "for pre, sen in zip(NB_predictions_bow[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RBhQJ96ftR5"
   },
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dtree_model_bow = DecisionTreeClassifier(max_depth = 5).fit(X_tr_bow.toarray() ,y_tr) \n",
    "dtree_predictions_bow = dtree_model_bow.predict(X_test_bow.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtxkcvIsXZjx",
    "outputId": "b94e1aca-34eb-4975-9e6c-9357ebea4547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   340.0   143.0 \n",
      "        pay   308.0   357.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.26\n",
      "Precision: 0.29\n",
      "Recall: 0.29\n",
      "F1: 0.27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.23      0.27      1495\n",
      "           1       0.34      0.16      0.22      2228\n",
      "           2       0.30      0.22      0.25      1989\n",
      "           3       0.34      0.33      0.33      1398\n",
      "           4       0.17      0.50      0.25      1117\n",
      "\n",
      "    accuracy                           0.26      8227\n",
      "   macro avg       0.29      0.29      0.27      8227\n",
      "weighted avg       0.30      0.26      0.26      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Positive\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Positive\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Positive\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Positive\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Positive\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Extremely Negative\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Positive\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Positive\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_bow.toarray(), y_test)\n",
    "for pre, sen in zip(dtree_predictions_bow[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1deDt7EXxFX"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "OneVsRestClassifier_model_bow = OneVsRestClassifier(LinearSVC(random_state=10))\n",
    "OneVsRestClassifier_model_bow.fit(X_tr_bow.toarray() ,y_tr)\n",
    "OneVsRestClassifier_model_bow_predictions_bow = OneVsRestClassifier_model_bow.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTR1j3TTmZ7X",
    "outputId": "06ca39ac-8c72-4b7c-f9df-2125f896f5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   340.0   143.0 \n",
      "        pay   308.0   357.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.26\n",
      "Precision: 0.29\n",
      "Recall: 0.29\n",
      "F1: 0.27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.23      0.27      1495\n",
      "           1       0.34      0.16      0.22      2228\n",
      "           2       0.30      0.22      0.25      1989\n",
      "           3       0.34      0.33      0.33      1398\n",
      "           4       0.17      0.50      0.25      1117\n",
      "\n",
      "    accuracy                           0.26      8227\n",
      "   macro avg       0.29      0.29      0.27      8227\n",
      "weighted avg       0.30      0.26      0.26      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Negative\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Positive\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Extremely Positive\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Negative\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Extremely Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Negative\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Extremely Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Extremely Negative\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Positive\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Negative\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_bow.toarray(), y_test)\n",
    "for pre, sen in zip(OneVsRestClassifier_model_bow_predictions_bow[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJJmYBt4YBwd"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "OneVsOneClassifier_model_bow = OneVsOneClassifier(LinearSVC(random_state=10))\n",
    "OneVsOneClassifier_model_bow.fit(X_tr_bow.toarray() ,y_tr)\n",
    "OneVsOneClassifier_model_bow_predictions_bow = OneVsOneClassifier_model_bow.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd7zIqpmYELd",
    "outputId": "b7dbab33-0633-4498-d88a-b4dd2dd15ca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   340.0   143.0 \n",
      "        pay   308.0   357.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.26\n",
      "Precision: 0.29\n",
      "Recall: 0.29\n",
      "F1: 0.27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.23      0.27      1495\n",
      "           1       0.34      0.16      0.22      2228\n",
      "           2       0.30      0.22      0.25      1989\n",
      "           3       0.34      0.33      0.33      1398\n",
      "           4       0.17      0.50      0.25      1117\n",
      "\n",
      "    accuracy                           0.26      8227\n",
      "   macro avg       0.29      0.29      0.27      8227\n",
      "weighted avg       0.30      0.26      0.26      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Negative\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Positive\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Extremely Positive\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Negative\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Extremely Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Positive\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Extremely Negative\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Positive\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Positive\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_bow.toarray(), y_test)\n",
    "for pre, sen in zip(OneVsOneClassifier_model_bow_predictions_bow[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdnAA3xqYGpm"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "OutputCodeClassifier_model_bow = OutputCodeClassifier(LinearSVC(random_state=1))\n",
    "OutputCodeClassifier_model_bow.fit(X_tr_bow.toarray() ,y_tr)\n",
    "OutputCodeClassifier_model_bow_predictions_bow = OutputCodeClassifier_model_bow.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rb7gHlSiYIil",
    "outputId": "e43386db-43d7-48e1-e454-07b9dbe6ff2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   340.0   143.0 \n",
      "        pay   308.0   357.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.26\n",
      "Precision: 0.29\n",
      "Recall: 0.29\n",
      "F1: 0.27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.23      0.27      1495\n",
      "           1       0.34      0.16      0.22      2228\n",
      "           2       0.30      0.22      0.25      1989\n",
      "           3       0.34      0.33      0.33      1398\n",
      "           4       0.17      0.50      0.25      1117\n",
      "\n",
      "    accuracy                           0.26      8227\n",
      "   macro avg       0.29      0.29      0.27      8227\n",
      "weighted avg       0.30      0.26      0.26      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Negative\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Positive\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Extremely Positive\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Negative\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Extremely Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Positive\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Extremely Negative\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Positive\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Negative\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_bow.toarray(), y_test)\n",
    "for pre, sen in zip(OutputCodeClassifier_model_bow_predictions_bow[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THnPVe70tlIJ",
    "outputId": "72c367fc-d9f0-4bb0-bd4e-9c402bb1a9a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisticregression_model_bow = LogisticRegression()\n",
    "logisticregression_model_bow.fit(X_tr_bow.toarray() ,y_tr)\n",
    "logisticregression_model_bow_predictions = logisticregression_model_bow.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7j6JqhXZt4RP",
    "outputId": "d5022c85-e88a-4ffe-eb9a-bf4ebc362633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   340.0   143.0 \n",
      "        pay   308.0   357.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.26\n",
      "Precision: 0.29\n",
      "Recall: 0.29\n",
      "F1: 0.27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.23      0.27      1495\n",
      "           1       0.34      0.16      0.22      2228\n",
      "           2       0.30      0.22      0.25      1989\n",
      "           3       0.34      0.33      0.33      1398\n",
      "           4       0.17      0.50      0.25      1117\n",
      "\n",
      "    accuracy                           0.26      8227\n",
      "   macro avg       0.29      0.29      0.27      8227\n",
      "weighted avg       0.30      0.26      0.26      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Negative\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Positive\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Positive\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Negative\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Extremely Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Positive\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Extremely Negative\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Positive\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Negative\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_bow.toarray(), y_test)\n",
    "for pre, sen in zip(logisticregression_model_bow_predictions[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WD-vN61sb8o"
   },
   "source": [
    "# TF-IDF Vectorizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzJHwx-msuY-"
   },
   "source": [
    "creating the transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YUlwFKSscNn",
    "outputId": "fa32aee4-0638-42fc-a7a4-8ea1ec3514b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tfidf_vectorizer = TfidfVectorizer()\n",
    "Tfidf_vectorizer.fit(X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g3kqClDswql"
   },
   "source": [
    "Transforming data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9Yajcwzsw00"
   },
   "outputs": [],
   "source": [
    "# transform training data\n",
    "X_tr_tf = Tfidf_vectorizer.transform(X_tr)\n",
    "\n",
    "# transform validation data\n",
    "X_val_tf = Tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "# transform test data\n",
    "X_test_tf = Tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnP-rpihU_tH"
   },
   "source": [
    "# TF_IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jECceRSMVEVp"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "NB_model_tf = GaussianNB().fit(X_tr_tf.toarray() ,y_tr)\n",
    "NB_model_tf_predictions_tf = NB_model_tf.predict(X_test_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCsGPa8YVOJO",
    "outputId": "2a00e77f-fabf-4006-dfb5-827844e1c520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   336.0   162.0 \n",
      "        pay   368.0   396.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.25\n",
      "Precision: 0.28\n",
      "Recall: 0.27\n",
      "F1: 0.26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.22      0.25      1495\n",
      "           1       0.33      0.18      0.23      2228\n",
      "           2       0.29      0.22      0.25      1989\n",
      "           3       0.32      0.28      0.30      1398\n",
      "           4       0.17      0.46      0.24      1117\n",
      "\n",
      "    accuracy                           0.25      8227\n",
      "   macro avg       0.28      0.27      0.26      8227\n",
      "weighted avg       0.29      0.25      0.25      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Extremely Negative\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Extremely Negative\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Extremely Negative\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Neutral\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Extremely Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Neutral\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Neutral\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Neutral\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Extremely Negative\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_tf.toarray(), y_test)\n",
    "for pre, sen in zip(NB_model_tf_predictions_tf[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hlec0l0X8Nd"
   },
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dtree_model_tf = DecisionTreeClassifier(max_depth = 5).fit(X_tr_tf.toarray() ,y_tr) \n",
    "dtree_predictions_tf = dtree_model_tf.predict(X_test_tf.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlT1aKRfYOiE",
    "outputId": "acf51eaa-ac1a-4ec5-ff64-ec3ffc38621f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   336.0   162.0 \n",
      "        pay   368.0   396.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.25\n",
      "Precision: 0.28\n",
      "Recall: 0.27\n",
      "F1: 0.26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.22      0.25      1495\n",
      "           1       0.33      0.18      0.23      2228\n",
      "           2       0.29      0.22      0.25      1989\n",
      "           3       0.32      0.28      0.30      1398\n",
      "           4       0.17      0.46      0.24      1117\n",
      "\n",
      "    accuracy                           0.25      8227\n",
      "   macro avg       0.28      0.27      0.26      8227\n",
      "weighted avg       0.29      0.25      0.25      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Positive\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Positive\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Positive\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Positive\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Positive\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Extremely Negative\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Positive\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Positive\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_tf.toarray(), y_test)\n",
    "for pre, sen in zip(dtree_predictions_tf[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RK_A-FIDnOdy"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "OneVsRestClassifier_model_tf = OneVsRestClassifier(LinearSVC(random_state=1))\n",
    "OneVsRestClassifier_model_tf.fit(X_tr_tf.toarray() ,y_tr)\n",
    "OneVsRestClassifier_model_bow_predictions_tf = OneVsRestClassifier_model_tf.predict(X_test_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExCSfO4Qneub",
    "outputId": "a22ffb18-a4b6-44ad-e400-2d2fd78bc5cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   336.0   162.0 \n",
      "        pay   368.0   396.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.25\n",
      "Precision: 0.28\n",
      "Recall: 0.27\n",
      "F1: 0.26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.22      0.25      1495\n",
      "           1       0.33      0.18      0.23      2228\n",
      "           2       0.29      0.22      0.25      1989\n",
      "           3       0.32      0.28      0.30      1398\n",
      "           4       0.17      0.46      0.24      1117\n",
      "\n",
      "    accuracy                           0.25      8227\n",
      "   macro avg       0.28      0.27      0.26      8227\n",
      "weighted avg       0.29      0.25      0.25      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Negative\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Positive\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Extremely Positive\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Negative\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Extremely Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Positive\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Extremely Negative\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Positive\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Negative\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_tf.toarray(), y_test)\n",
    "for pre, sen in zip(OneVsRestClassifier_model_bow_predictions_tf[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbwhlY4fnlO9"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "OneVsOneClassifier_model_tf = OneVsOneClassifier(LinearSVC(random_state=1))\n",
    "OneVsOneClassifier_model_tf.fit(X_tr_tf.toarray() ,y_tr)\n",
    "OneVsOneClassifier_model_tf_predictions = OneVsOneClassifier_model_tf.predict(X_test_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8T1kyiNnlWH",
    "outputId": "03a9e1d1-bcc3-4cb0-f12b-df44024e3027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   336.0   162.0 \n",
      "        pay   368.0   396.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.25\n",
      "Precision: 0.28\n",
      "Recall: 0.27\n",
      "F1: 0.26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.22      0.25      1495\n",
      "           1       0.33      0.18      0.23      2228\n",
      "           2       0.29      0.22      0.25      1989\n",
      "           3       0.32      0.28      0.30      1398\n",
      "           4       0.17      0.46      0.24      1117\n",
      "\n",
      "    accuracy                           0.25      8227\n",
      "   macro avg       0.28      0.27      0.26      8227\n",
      "weighted avg       0.29      0.25      0.25      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Negative\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Positive\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Extremely Positive\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Negative\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Extremely Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Positive\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Extremely Negative\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Positive\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Negative\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_tf.toarray(), y_test)\n",
    "for pre, sen in zip(OneVsOneClassifier_model_tf_predictions[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pogp-I39ufSq",
    "outputId": "b39ae51b-ab2d-4155-b363-e76893b9a7fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisticregression_model_tf = LogisticRegression()\n",
    "logisticregression_model_tf.fit(X_tr_tf.toarray() ,y_tr)\n",
    "logisticregression_model_tf_predictions = logisticregression_model_tf.predict(X_test_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DPdrNNauffV",
    "outputId": "877810d8-1557-477b-f999-ed1e6ddc413a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Not pay     pay \n",
      "    Not pay   336.0   162.0 \n",
      "        pay   368.0   396.0 \n",
      "Confusion matrix None\n",
      "Accuracy: 0.25\n",
      "Precision: 0.28\n",
      "Recall: 0.27\n",
      "F1: 0.26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.22      0.25      1495\n",
      "           1       0.33      0.18      0.23      2228\n",
      "           2       0.29      0.22      0.25      1989\n",
      "           3       0.32      0.28      0.30      1398\n",
      "           4       0.17      0.46      0.24      1117\n",
      "\n",
      "    accuracy                           0.25      8227\n",
      "   macro avg       0.28      0.27      0.26      8227\n",
      "weighted avg       0.29      0.25      0.25      8227\n",
      "\n",
      "test :feeling angry going supermarket also seeing images vulnerable wrote old man sure #covid #coronavirus, \n",
      "result : Negative\n",
      "test :english working suppliers restock get much food many homes possible however experiencing high demand currently apologise receive exact items ordered info please see kelsie, \n",
      "result : Positive\n",
      "test :support food service workers tipping extra signing please, \n",
      "result : Extremely Positive\n",
      "test :abrupt national #lockdown puts #jobs risk #textiles #shoemaking #jewellery consumer #goods sectors #covid via, \n",
      "result : Negative\n",
      "test :love seeing creative ways local retailers adapting challenging time #coronavirus #toiletpaper #toiletpapercrisis #toiletpapergate #takeouttuesday, \n",
      "result : Extremely Positive\n",
      "test :prices important staples like #rice #wheat surging #coronavirus upends supply chains source bloomberg, \n",
      "result : Positive\n",
      "test :ye explain prices rising items bush internet radio jumped would hope ye increasing prices #coronacrisis receipt prove, \n",
      "result : Positive\n",
      "test :#petrolprice #coronavirus one silver lining current covid crisis international oil prices plummeted despite rand losses set result massive savings pumps april, \n",
      "result : Extremely Negative\n",
      "test :please confirm whether dis chem statement per attached pictures true prices doubled period corona virus actually trying profit pandemic, \n",
      "result : Positive\n",
      "test :danforth home supermarket helping older risk customers covid outbreak, \n",
      "result : Positive\n"
     ]
    }
   ],
   "source": [
    "evaluate(NB_model_bow, X_test_tf.toarray(), y_test)\n",
    "for pre, sen in zip(logisticregression_model_tf_predictions[:10], X_test[:10]):\n",
    "  print(f\"test :{sen}, \\nresult : {dict[pre]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EyHDwJ1p2A4"
   },
   "source": [
    "# Deeplearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SNUFTZ_p4U5"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lrku_dMV27A9"
   },
   "outputs": [],
   "source": [
    "X_tr = tokenizer.texts_to_sequences(X_tr)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k2ijt3d3tds"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EXFU2Iq4cpy",
    "outputId": "e9317c3c-85e8-4660-d756-c3f96668598c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   5 1071  361   45 1071   40  735 1676 1533 1242  609 2219 1676  735\n",
      "  178  637 1317  153   71 2039   49 1044  204   74   40 1173    2    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_tr = pad_sequences(X_tr, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_tr[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS5nf7ii5GcS",
    "outputId": "2454bdb1-14d6-41f0-f50a-09662a4fbd1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          3151200   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 3,332,605\n",
      "Trainable params: 3,332,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding, Bidirectional, Dropout\n",
    "\n",
    "Embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, Embedding_dim, input_length=maxlen))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(Embedding_dim)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy', precision_m, recall_m, f1_m])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJ53JFM8gD10"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/NLP Homework/tokenizer.pkl', 'wb') as f:\n",
    "  pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "bQsBtXBQ6RmM",
    "outputId": "40e143c0-2ff0-4707-a8e7-29ca210a7c76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2304/2304 [==============================] - 358s 145ms/step - loss: 1.3310 - accuracy: 0.4161 - precision_m: 61042162.9406 - recall_m: 1.5699 - f1_m: 2.6212 - val_loss: 0.8573 - val_accuracy: 0.6837 - val_precision_m: 1.4139 - val_recall_m: 1.3640 - val_f1_m: 1.3661\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.13366, saving model to /content/drive/MyDrive/Colab Notebooks/NLP Homework/LanguageModel.hdf5\n",
      "Epoch 2/5\n",
      "2304/2304 [==============================] - 327s 142ms/step - loss: 0.7631 - accuracy: 0.7260 - precision_m: 1.2012 - recall_m: 1.3021 - f1_m: 1.2372 - val_loss: 0.8092 - val_accuracy: 0.7050 - val_precision_m: 1.1759 - val_recall_m: 1.2812 - val_f1_m: 1.2152\n",
      "\n",
      "Epoch 00002: loss improved from 1.13366 to 0.76061, saving model to /content/drive/MyDrive/Colab Notebooks/NLP Homework/LanguageModel.hdf5\n",
      "Epoch 3/5\n",
      "2304/2304 [==============================] - 330s 143ms/step - loss: 0.6591 - accuracy: 0.7697 - precision_m: 1.0884 - recall_m: 1.2284 - f1_m: 1.1441 - val_loss: 0.7805 - val_accuracy: 0.7352 - val_precision_m: 1.1188 - val_recall_m: 1.2331 - val_f1_m: 1.1616\n",
      "\n",
      "Epoch 00003: loss improved from 0.76061 to 0.67260, saving model to /content/drive/MyDrive/Colab Notebooks/NLP Homework/LanguageModel.hdf5\n",
      "Epoch 4/5\n",
      "2304/2304 [==============================] - 326s 141ms/step - loss: 0.5921 - accuracy: 0.7996 - precision_m: 1.0403 - recall_m: 1.1954 - f1_m: 1.1036 - val_loss: 0.7963 - val_accuracy: 0.7174 - val_precision_m: 1.0914 - val_recall_m: 1.2132 - val_f1_m: 1.1383\n",
      "\n",
      "Epoch 00004: loss improved from 0.67260 to 0.61109, saving model to /content/drive/MyDrive/Colab Notebooks/NLP Homework/LanguageModel.hdf5\n",
      "Epoch 5/5\n",
      "2304/2304 [==============================] - 328s 143ms/step - loss: 0.5225 - accuracy: 0.8231 - precision_m: 1.0119 - recall_m: 1.1659 - f1_m: 1.0742 - val_loss: 0.8539 - val_accuracy: 0.7212 - val_precision_m: 1.0199 - val_recall_m: 1.1727 - val_f1_m: 1.0817\n",
      "\n",
      "Epoch 00005: loss improved from 0.61109 to 0.54543, saving model to /content/drive/MyDrive/Colab Notebooks/NLP Homework/LanguageModel.hdf5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c422b0b850ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     callbacks=desired_callbacks)\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Accuracy: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "filepath = \"/content/drive/MyDrive/Colab Notebooks/NLP Homework/LanguageModel.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]\n",
    "history = model.fit(X_tr, np.array(y_tr),\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_val, np.array(y_val)),\n",
    "                    batch_size=10,\n",
    "                    callbacks=desired_callbacks)\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIIb0-h71nwd",
    "outputId": "adb47587-2281-4323-8783-9ba9a6e91f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0.8346096277236938, 0.72857666015625, 1.0305896997451782, 1.1803340911865234, 1.0979121923446655]\n",
      "258/258 [==============================] - 13s 49ms/step - loss: 0.8346 - accuracy: 0.7286 - precision_m: 1.0306 - recall_m: 1.1803 - f1_m: 1.0979\n",
      "Test:  [0.8346096277236938, 0.72857666015625, 1.0305896997451782, 1.1803340911865234, 1.0979121923446655]\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(X_tr, y_tr)\n",
    "print(\"Train: {}\".format(metrics))\n",
    "metrics = model.evaluate(X_test, np.array(y_test))\n",
    "print(\"Test:  {}\".format(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmMumIW46tSS"
   },
   "outputs": [],
   "source": [
    "model.save(\"/content/drive/MyDrive/Colab Notebooks/NLP Homework/language_model_classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuDFZPpr9lI6"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "my_model = load_model(\"/content/drive/MyDrive/Colab Notebooks/NLP Homework/language_model_classification.h5\")\n",
    "history = my_model.fit(X_tr, np.array(y_tr),\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, np.array(y_test)),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLUF_tAw19bw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Homework_part_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
